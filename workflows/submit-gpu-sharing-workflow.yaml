apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: gpu-allocation-
spec:
  ttlStrategy:
    secondsAfterCompletion: 3600
  serviceAccountName: argo-workflow
  volumeClaimTemplates:
  - metadata:
      name: shared-volume
    spec:
      accessModes: [ "ReadWriteMany" ]
      resources:
        requests:
          storage: 1M
  volumes:
  - name: nvidia-cuda-mps-control
    hostPath:
      path: /usr/bin/nvidia-cuda-mps-control
      type: File
  - name: nvidia-mps
    hostPath:
      path: /tmp/nvidia-mps
      type: DirectoryOrCreate
  - name: nvidia-mps-log
    hostPath:
      path: /var/log/nvidia-mps
      type: DirectoryOrCreate
  entrypoint: main
  parallelism: 2
  arguments:
    parameters:
    - name: secret
      value: argo-workflows-admin.service-account-token
    - name: argo-server-url
      value: http://argo-workflows-server.argo.svc.cluster.local:2746/api/v1/workflows
    - name: namespace
      value: default
    - name: gpus
      value: 1
    - name: mps
      value: enabled
    - name: workflow-name
      value: gpu-sharing-workflow-template
    - name: interval
      value: 1  # sec
  templates:
  - name: main
    dag:
      tasks:
      - name: gpu-allocation
        template: gpu-allocation
      - name: init
        template: init
      - name: trigger-gpu-sharing-workflow
        template: trigger-gpu-sharing-workflow
        dependencies: [init]
        arguments:
          parameters:
          - name: gpu-uuids
            value: "{{tasks.init.outputs.parameters.gpu-uuids}}"
          - name: node-name
            value: "{{tasks.init.outputs.parameters.node-name}}"
      - name: tear-down
        template: tear-down
        dependencies: [trigger-gpu-sharing-workflow]
        continueOn:
          failed: true


  - name: gpu-allocation
    podSpecPatch: '{"containers":[{"name":"main", "resources":{"limits":{"nvidia.com/gpu": "{{workflow.parameters.gpus}}" }}}]}'
    script:
      image: nvidia/cuda:12.1.0-base-ubuntu18.04
      securityContext:
        privileged: true  # for MPS
      volumeMounts:
      - name: shared-volume
        mountPath: /mnt/shared
      - name: nvidia-cuda-mps-control
        mountPath: /host/bin/nvidia-cuda-mps-control
      - name: nvidia-mps
        mountPath: /tmp/nvidia-mps
      - name: nvidia-mps-log
        mountPath: /var/log/nvidia-mps
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
      command: [sh]
      source: |
        echo "GPU UUID: $NVIDIA_VISIBLE_DEVICES / NODE NAME: $NODE_NAME"

        # mps activation
        if [ "{{workflow.parameters.mps}}" = "enabled"  ]; then
          echo "mps activation start"
          nvidia-smi -c EXCLUSIVE_PROCESS
          echo "set GPU $NVIDIA_VISIBLE_DEVICES mode as EXECLUSIVE_PROCESS"
          nvidia-cuda-mps-control -d
          echo "MPS enabled"
          echo $(ps -ef | grep mps)
        fi

        # pass all required information for gpu uses.
        echo -n $NVIDIA_VISIBLE_DEVICES >> /mnt/shared/gpu-uuids
        echo -n $NODE_NAME >> /mnt/shared/node-name

        # wait for the gpu sharing workflow terminated...
        while [ ! -f "/mnt/shared/terminated" ]; do
          echo "Sleeping for {{workflow.parameters.interval}} seconds until the workflow terminated..."
          sleep {{workflow.parameters.interval}}
        done

        # mps deactivation
        if [ "{{workflow.parameters.mps}}" = "enabled"  ]; then
          echo "set GPU $NVIDIA_VISIBLE_DEVICES mode as DEFAULT"
          nvidia-smi -c DEFAULT
          echo "MPS will be disabled after this pod is terminated"
        fi

  - name: init
    script:
      image: alpine:3.18
      volumeMounts:
      - name: shared-volume
        mountPath: /mnt/shared
      command: [sh]
      source: |
        gpu_uuids_path="/mnt/shared/gpu-uuids"
        node_name_path="/mnt/shared/node-name"
        while [ ! -f $gpu_uuids_path ] || [ ! -f $node_name_path ]; do
          echo "Sleeping for {{workflow.parameters.interval}} seconds until the gpu(s) allocated..."
          sleep {{workflow.parameters.interval}}
        done
        cat $gpu_uuids_path
        cp $gpu_uuids_path /tmp/gpu_uuids
        cat $node_name_path
        cp $node_name_path /tmp/node_name
    outputs:
      parameters:
      - name: gpu-uuids
        valueFrom:
          path: /tmp/gpu_uuids
      - name: node-name
        valueFrom:
          path: /tmp/node_name

  - name: trigger-gpu-sharing-workflow
    inputs:
      parameters:
      - name: gpu-uuids
      - name: node-name
    script:
      image: python:3.10-alpine
      command: [python]
      args: ["-u"]
      env:
      - name: ARGO_TOKEN
        valueFrom:
          secretKeyRef:
            name: "{{workflow.parameters.secret}}"
            key: token
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
      source: |
        import os
        import json
        import time
        import urllib
        import urllib.request

        gpu_uuids = "{{inputs.parameters.gpu-uuids}}"
        node_name = "{{inputs.parameters.node-name}}"
        token = os.getenv("ARGO_TOKEN")
        print(f"GPU:{gpu_uuids} / node:{node_name}")

        # workflow submission preparation.
        data = {}
        data["resourceKind"] = "WorkflowTemplate"
        data["resourceName"] = "{{workflow.parameters.workflow-name}}"
        data["submitOptions"] = {"parameters": ["gpu-uuids=" + gpu_uuids, "node-name=" + node_name]}
        url = "{{workflow.parameters.argo-server-url}}/{{workflow.parameters.namespace}}/submit"
        req = urllib.request.Request(url, str(json.dumps(data)).encode("utf-8"))
        req.add_header("Authorization", "Bearer " + token)

        # trigger the workflow.
        resp = json.loads(urllib.request.urlopen(req).read().decode("utf-8"))
        workflow_name = resp["metadata"]["name"]
        print(workflow_name + " triggered")

        # workflow status check preparation.
        url = f"{{workflow.parameters.argo-server-url}}/{{workflow.parameters.namespace}}/{workflow_name}"
        req, status = urllib.request.Request(url), None
        req.add_header("Authorization", "Bearer " + token)

        # wait for the workflow done.
        while True:
            resp = json.loads(urllib.request.urlopen(req).read().decode("utf-8"))
            if "status" in resp and "phase" in resp["status"]:
                status = resp["status"]["phase"]
                print(status)
            if status and status in ("Succeeded", "Failed", "Error"):
                break
            time.sleep({{workflow.parameters.interval}})

  - name: tear-down
    script:
      image: alpine:3.18
      volumeMounts:
      - name: shared-volume
        mountPath: /mnt/shared
      command: [sh]
      source: |
        touch /mnt/shared/terminated
